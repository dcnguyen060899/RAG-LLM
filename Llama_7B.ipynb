{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOkdWnl1jn5NRE4ntGbd2F2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dcnguyen060899/RAG-LLM/blob/main/Llama_7B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Documentation for research, design and implementation\n",
        "\n",
        "This report will provide an in-depth analysis of the code for a Streamlit-based web application, focusing on its integration with language models and document retrieval systems. The primary objective of this application is to create a sophisticated interface for uploading, processing, and querying PDF documents using the LLaMa 2 7B language model. The application exemplifies a blend of various advanced technologies, including Streamlit, Transformers, and custom document handling mechanisms.\n",
        "\n",
        "### 1. Overview of the Application\n",
        "\n",
        "The application is designed as a web interface using Streamlit, a popular framework for building data applications. It allows users to upload PDF documents, which are then processed and made searchable through natural language queries. The core of the application revolves around the integration of a language model, specifically the LLaMa 2 7B model, for processing and responding to user queries.\n",
        "\n",
        "### 2. Key Components and Their Roles\n",
        "\n",
        "#### 2.1 Streamlit Interface\n",
        "Streamlit is utilized to create a user-friendly web interface. It handles file uploads, displays query results, and manages user interactions.\n",
        "\n",
        "#### 2.2 Transformers and LLaMa 2 7B Model\n",
        "The application employs the `transformers` library to load the LLaMa 2 7B model. This model is pivotal for generating responses to user queries and processing text data.\n",
        "\n",
        "#### 2.3 Document Handling and Indexing\n",
        "The application incorporates functionality to read and index PDF documents, making them searchable. This feature is crucial for handling user-uploaded documents.\n",
        "\n",
        "### 3. Detailed Code Analysis\n",
        "\n",
        "#### 3.1 Import Statements\n",
        "The code begins with importing necessary libraries and modules. Key imports include Streamlit, Transformers for the language model, PyPDF for PDF processing, and various custom modules for embedding and indexing.\n",
        "\n",
        "#### 3.2 Tokenizer and Model Initialization\n",
        "The `get_tokenizer_model` function initializes the tokenizer and model for the LLaMa 2 7B model. These components are essential for processing natural language inputs.\n",
        "\n",
        "#### 3.3 Streamlit UI Components\n",
        "The code then sets up Streamlit UI elements such as text areas and buttons for user interaction.\n",
        "\n",
        "#### 3.4 LLaMa Model and Query Wrapper\n",
        "A `HuggingFaceLLM` object is instantiated, representing the LLaMa model integrated with a query wrapper for handling user inputs.\n",
        "\n",
        "#### 3.5 System Prompt Update Function\n",
        "The `update_system_prompt` function allows dynamic updating of the system prompt based on user input, demonstrating the application's interactive nature.\n",
        "\n",
        "#### 3.6 Embeddings and Service Context\n",
        "The application uses embeddings to represent document chunks and sets up a service context for managing these operations.\n",
        "\n",
        "#### 3.7 File Upload and Document Indexing\n",
        "The code provides functionality for users to upload PDF files, which are then saved and indexed for querying.\n",
        "\n",
        "#### 3.8 Streamlit Chat Interface\n",
        "A chat interface is created using Streamlit components, facilitating the interaction between the user and the application.\n",
        "\n",
        "#### 3.9 Query Handling\n",
        "The application processes user queries using the indexed documents and the LLaMa model, showcasing the retrieval-augmented aspect of the system.\n",
        "\n",
        "### 4. Implementation Challenges and Strategies\n",
        "\n",
        "#### 4.1 Integration of Diverse Technologies\n",
        "Combining Streamlit, Transformers, and custom indexing mechanisms was a significant challenge. The solution was to modularize the code and carefully manage dependencies.\n",
        "\n",
        "#### 4.2 Efficient Document Handling\n",
        "Efficiently processing and indexing PDF documents required careful planning. The use of PyPDF and custom indexing algorithms addressed this challenge.\n",
        "\n",
        "#### 4.3 User Interface Design\n",
        "Creating an intuitive and responsive user interface with Streamlit was essential. The strategy involved iterative design and user feedback.\n",
        "\n",
        "### 5. User-Driven Data Upload and Custom Retrieval\n",
        "\n",
        "#### 5.1 Purpose and Functionality\n",
        "In addition to processing and querying pre-existing documents, the application provides a unique feature where users can upload their data, such as resumes or candidate profiles. This functionality is particularly tailored for hiring managers or recruitment teams. They can upload a set of resumes and then use the application to find the most suitable candidates based on a specific job description and hiring guidelines.\n",
        "\n",
        "#### 5.2 Implementation Strategy\n",
        "The application employs a two-step process to facilitate this feature:\n",
        "\n",
        "1. **Data Upload and Processing**: Users can upload documents (e.g., resumes) via the Streamlit interface. Once uploaded, these documents are processed and indexed similarly to the initial set of PDFs. This process involves extracting text from the resumes and converting them into a searchable format, which is then added to the application’s document index.\n",
        "\n",
        "2. **Query-Based Retrieval-Augmented Generation (RAG)**: The core feature of the application is its ability to perform RAG based on user queries. When a hiring manager inputs job requirements or specific criteria, the application uses the LLaMa 2 7B model to understand and interpret these requirements. It then searches through the indexed resumes to find matches. The model's retrieval mechanism is designed to understand the context of the query (job description) and extract relevant information from the uploaded documents (candidate profiles), thereby suggesting the most fitting candidates.\n",
        "\n",
        "#### 5.3 Enhancing Candidate Selection\n",
        "The application's RAG system can significantly streamline the hiring process. By automating the initial screening of candidates based on specific criteria, it reduces the time and effort required in candidate selection. This system is not just a keyword match but understands the nuances of the job description and the candidates' qualifications, leading to more accurate and relevant recommendations.\n",
        "\n",
        "#### 5.4 Challenges and Solutions\n",
        "- **Data Privacy and Security**: Handling personal data such as resumes necessitates stringent data privacy measures. The application must be designed with robust security protocols to ensure user data protection.\n",
        "- **Accurate Information Retrieval**: The success of this feature depends on the model's ability to accurately interpret and retrieve relevant information from a diverse set of documents. Continuous testing and fine-tuning of the model are essential to improve accuracy.\n",
        "\n",
        "#### 5.5 User Experience\n",
        "To maximize user satisfaction, the application provides a clear and intuitive interface for uploading documents and inputting queries. Feedback mechanisms are also in place to refine the model based on user interactions and improve the overall effectiveness of the candidate selection process.\n",
        "\n",
        "### 6. Conclusion\n",
        "This extension of the application showcases its adaptability and potential in real-world scenarios like recruitment. By enabling users to upload their data and utilizing a sophisticated RAG system, the application not only serves as a query-response tool but also becomes a powerful aid in decision-making processes like hiring. The integration of user-uploaded data with the LLaMa 2 7B model's retrieval and generation capabilities marks a significant advancement in the application's utility and effectiveness."
      ],
      "metadata": {
        "id": "rUUOPSBnddN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "oDhR3VhIC952",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a20aee77-5163-4a61-e050-c45a86e7be0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok==4.1.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTjxoFI7F1N3",
        "outputId": "dc98beee-9f78-4e2d-c0b4-fd7404208360"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok==4.1.1\n",
            "  Downloading pyngrok-4.1.1.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pyngrok==4.1.1) (0.18.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyngrok==4.1.1) (6.0.1)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-4.1.1-py3-none-any.whl size=15963 sha256=928a5824582b97b676998cfa0f36e3aa6916bc9c7d78a1e589f3336daefb72a1\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/7c/4c/632fba2ea8e88d8890102eb07bc922e1ca8fa14db5902c91a8\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-4.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81BnGEgqDn-Z",
        "outputId": "b4ac73d8-06a1-47a9-cd2d-48b5fea57bc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain einops accelerate bitsandbytes scipy xformers sentencepiece llama-index llama_hub sentence-transformers pypdf streamlit transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eJa_CP3A8cI",
        "outputId": "ecde93dc-5208-435d-b415-d47118bc98cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.0.344-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting einops\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate\n",
            "  Downloading accelerate-0.25.0-py3-none-any.whl (265 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.41.2.post2-py3-none-any.whl (92.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.11.4)\n",
            "Collecting xformers\n",
            "  Downloading xformers-0.0.22.post7-cp310-cp310-manylinux2014_x86_64.whl (211.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.8/211.8 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-index\n",
            "  Downloading llama_index-0.9.10-py3-none-any.whl (917 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m917.6/917.6 kB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama_hub\n",
            "  Downloading llama_hub-0.0.50-py3-none-any.whl (24.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.4/24.4 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-3.17.1-py3-none-any.whl (277 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m277.6/277.6 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting streamlit\n",
            "  Downloading streamlit-1.29.0-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m115.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.1)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-core<0.1,>=0.0.8 (from langchain)\n",
            "  Downloading langchain_core-0.0.8-py3-none-any.whl (177 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langsmith<0.1.0,>=0.0.63 (from langchain)\n",
            "  Downloading langsmith-0.0.68-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.19.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Collecting aiostream<0.6.0,>=0.5.2 (from llama-index)\n",
            "  Downloading aiostream-0.5.2-py3-none-any.whl (39 kB)\n",
            "Collecting beautifulsoup4<5.0.0,>=4.12.2 (from llama-index)\n",
            "  Downloading beautifulsoup4-4.12.2-py3-none-any.whl (142 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.0/143.0 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting deprecated>=1.2.9.3 (from llama-index)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting httpx (from llama-index)\n",
            "  Downloading httpx-0.25.2-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index) (1.5.8)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index) (3.8.1)\n",
            "Collecting openai>=1.1.0 (from llama-index)\n",
            "  Downloading openai-1.3.7-py3-none-any.whl (221 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m221.4/221.4 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index) (1.5.3)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index)\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect>=0.8.0 (from llama-index)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting html2text (from llama_hub)\n",
            "  Downloading html2text-2020.1.16-py3-none-any.whl (32 kB)\n",
            "Collecting pyaml<24.0.0,>=23.9.7 (from llama_hub)\n",
            "  Downloading pyaml-23.9.7-py3-none-any.whl (23 kB)\n",
            "Collecting retrying (from llama_hub)\n",
            "  Downloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.16.0+cu118)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.3.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: importlib-metadata<7,>=1.4 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.8.0)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.4.0)\n",
            "Requirement already satisfied: protobuf<5,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=6.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.7.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.8.2)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.7.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: tzlocal<6,>=1.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.2)\n",
            "Collecting validators<1,>=0.2 (from streamlit)\n",
            "  Downloading validators-0.22.0-py3-none-any.whl (26 kB)\n",
            "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
            "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m121.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.2)\n",
            "Collecting watchdog>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-3.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.6)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.2.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.2->llama-index) (2.5)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.9.3->llama-index) (1.14.1)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7,>=1.4->streamlit) (3.17.0)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index) (1.3.2)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama-index) (1.7.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index) (2023.11.17)\n",
            "Collecting httpcore==1.* (from httpx->llama-index)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->llama-index)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3,>=2.7.3->streamlit) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.16.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.11.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.31.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.13.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=59ea8fc2e2df65fa61287c408ff6b43e7755f2291c9ae4f262454108341811e6\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: sentencepiece, bitsandbytes, watchdog, validators, smmap, retrying, pypdf, pyaml, mypy-extensions, marshmallow, jsonpointer, html2text, h11, einops, deprecated, beautifulsoup4, aiostream, typing-inspect, tiktoken, pydeck, langsmith, jsonpatch, httpcore, gitdb, xformers, langchain-core, httpx, gitpython, dataclasses-json, accelerate, openai, langchain, streamlit, sentence-transformers, llama-index, llama_hub\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.11.2\n",
            "    Uninstalling beautifulsoup4-4.11.2:\n",
            "      Successfully uninstalled beautifulsoup4-4.11.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-0.25.0 aiostream-0.5.2 beautifulsoup4-4.12.2 bitsandbytes-0.41.2.post2 dataclasses-json-0.6.3 deprecated-1.2.14 einops-0.7.0 gitdb-4.0.11 gitpython-3.1.40 h11-0.14.0 html2text-2020.1.16 httpcore-1.0.2 httpx-0.25.2 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.344 langchain-core-0.0.8 langsmith-0.0.68 llama-index-0.9.10 llama_hub-0.0.50 marshmallow-3.20.1 mypy-extensions-1.0.0 openai-1.3.7 pyaml-23.9.7 pydeck-0.8.1b0 pypdf-3.17.1 retrying-1.3.4 sentence-transformers-2.2.2 sentencepiece-0.1.99 smmap-5.0.1 streamlit-1.29.0 tiktoken-0.5.1 typing-inspect-0.9.0 validators-0.22.0 watchdog-3.0.0 xformers-0.0.22.post7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define variable to hold llama2 weights naming\n",
        "name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "# Set auth token variable from hugging face\n",
        "auth_token = \"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\""
      ],
      "metadata": {
        "id": "cOvVV-4kG3V3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(name, cache_dir='/content/drive/My Drive/LLM Deployment/LLM Deployment/', use_auth_token=auth_token)"
      ],
      "metadata": {
        "id": "14n25JBSHZ0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile app_test.py\n",
        "\n",
        "import streamlit as st\n",
        "\n",
        "# Import transformer classes for generaiton\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
        "# Import torch for datatype attributes\n",
        "import torch\n",
        "# Import the prompt wrapper...but for llama index\n",
        "from llama_index.prompts.prompts import SimpleInputPrompt\n",
        "# Import the llama index HF Wrapper\n",
        "from llama_index.llms import HuggingFaceLLM\n",
        "# Bring in embeddings wrapper\n",
        "from llama_index.embeddings import LangchainEmbedding\n",
        "# Bring in HF embeddings - need these to represent document chunks\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "# Bring in stuff to change service context\n",
        "from llama_index import set_global_service_context\n",
        "from llama_index import ServiceContext\n",
        "# Import deps to load documents\n",
        "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
        "from pathlib import Path\n",
        "import pypdf\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Define variable to hold llama2 weights namingfiner\n",
        "name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "# Set auth token variable from hugging face\n",
        "auth_token = \"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\"\n",
        "\n",
        "@st.cache_resource\n",
        "def get_tokenizer_model():\n",
        "    # Create tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(name, cache_dir='/content/drive/My Drive/LLM Deployment/LLM Deployment/', use_auth_token=auth_token)\n",
        "\n",
        "    # Create model\n",
        "    model = AutoModelForCausalLM.from_pretrained(name, cache_dir='/content/drive/My Drive/LLM Deployment/LLM Deployment/'\n",
        "                            , use_auth_token=auth_token, torch_dtype=torch.float16,\n",
        "                            rope_scaling={\"type\": \"dynamic\", \"factor\": 2}, load_in_8bit=True)\n",
        "\n",
        "    return model, tokenizer\n",
        "model, tokenizer = get_tokenizer_model()\n",
        "\n",
        "# Initialize the SimpleInputPrompt with an empty template\n",
        "query_wrapper_prompt = SimpleInputPrompt(\"{query_str} [/INST]\")\n",
        "\n",
        "# Streamlit UI to let the user update the system prompt\n",
        "# Start with an empty string or a default prompt\n",
        "default_prompt = \"\"\n",
        "user_system_prompt = st.text_area(\"How can I best assist you?\", value=\"\", height=100)\n",
        "update_button = st.button('Request')\n",
        "\n",
        "# Initialize the llm object with a placeholder or default system prompt\n",
        "llm = HuggingFaceLLM(\n",
        "    context_window=4096,\n",
        "    max_new_tokens=256,\n",
        "    system_prompt=\"\",  # Placeholder if your initial prompt is empty\n",
        "    query_wrapper_prompt=query_wrapper_prompt,  # Placeholder string\n",
        "    model=model,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# Function to update the system prompt and reinitialize the LLM with the new prompt\n",
        "def update_system_prompt(new_prompt):\n",
        "    global llm\n",
        "    llm.system_prompt = new_prompt\n",
        "\n",
        "\n",
        "if update_button:\n",
        "    # Update the system prompt and reinitialize the LLM\n",
        "    update_system_prompt(user_system_prompt)\n",
        "    st.success('Requested')\n",
        "\n",
        "# Create and dl embeddings instance\n",
        "embeddings=LangchainEmbedding(\n",
        "    HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        ")\n",
        "\n",
        "# Create new service context instance\n",
        "service_context = ServiceContext.from_defaults(\n",
        "    chunk_size=1024,\n",
        "    llm=llm,\n",
        "    embed_model=embeddings\n",
        ")\n",
        "\n",
        "# And set the service context\n",
        "set_global_service_context(service_context)\n",
        "\n",
        "# Define a directory for storing uploaded files\n",
        "UPLOAD_DIRECTORY = \"/content/\"\n",
        "\n",
        "if not os.path.exists(UPLOAD_DIRECTORY):\n",
        "    os.makedirs(UPLOAD_DIRECTORY)\n",
        "\n",
        "st.title('PDF Upload and Query Interface')\n",
        "\n",
        "# File uploader allows user to add PDF\n",
        "uploaded_file = st.file_uploader(\"Upload PDF\", type=\"pdf\", accept_multiple_files=True)\n",
        "upload_button = st.button('Upload')\n",
        "\n",
        "if uploaded_file and upload_button:\n",
        "  for file in uploaded_file:\n",
        "  # Save the uploaded PDF to the directory\n",
        "    with open(os.path.join(UPLOAD_DIRECTORY, file.name), \"wb\") as f:\n",
        "      f.write(file.getbuffer())\n",
        "    st.success(\"File uploaded successfully.\")\n",
        "\n",
        "documents = SimpleDirectoryReader(UPLOAD_DIRECTORY).load_data()\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "\n",
        "\n",
        "# Setup index query engine using LLM\n",
        "query_engine = index.as_query_engine(streaming=True, similarity_top_k=1)\n",
        "\n",
        "# Create centered main title\n",
        "st.title('👔 HireMind 🧩')\n",
        "\n",
        "# setup a session to hold all the old prompt\n",
        "if 'messages' not in st.session_state:\n",
        "  st.session_state.messages = []\n",
        "\n",
        "# print out the history message\n",
        "for message in st.session_state.messages:\n",
        "  st.chat_message(message['role']).markdown(message['content'])\n",
        "\n",
        "\n",
        "# Create a text input box for the user\n",
        "# If the user hits enter\n",
        "prompt = st.chat_input('Input your prompt here')\n",
        "\n",
        "if prompt:\n",
        "  st.chat_message('user').markdown(prompt)\n",
        "  st.session_state.messages.append({'role': 'user', 'content': prompt})\n",
        "\n",
        "  response = query_engine.query(prompt)\n",
        "\n",
        "  st.chat_message('assistant').markdown(response)\n",
        "  st.session_state.messages.append(\n",
        "      {'role': 'assistant', 'content': response}\n",
        "  )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLomgVCNNVJf",
        "outputId": "de1c4079-5a71-4041-968f-f23e8f3dc864"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app_test.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Terminate open tunnels if exist\n",
        "ngrok.kill()\n",
        "ngrok.set_auth_token(\"2Y0hRfVfNK8adHVauZ0aEaqvM7w_7HYUBtmJ8DTrBZ4mq1D32\")\n",
        "\n",
        "# Set up a new tunnel\n",
        "public_url = ngrok.connect(port='8501')\n",
        "print('Streamlit URL:', public_url)\n",
        "\n",
        "!streamlit run app_test.py &\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juv4ysJgFrM7",
        "outputId": "567a8fc2-0b8f-4a9c-99ca-6bad02a47435"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit URL: http://673b-35-204-195-223.ngrok-free.app\n",
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to False.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://35.204.195.223:8501\u001b[0m\n",
            "\u001b[0m\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:671: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n",
            "Loading checkpoint shards: 100% 2/2 [01:22<00:00, 41.32s/it]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n",
            "2023-12-01 22:23:13.654229: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-01 22:23:13.654289: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-01 22:23:13.654335: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-01 22:23:15.368658: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XN5coenpdPIR"
      }
    }
  ]
}